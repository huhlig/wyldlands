//
// Copyright 2025-2026 Hans W. Uhlig. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

//! LLM-powered generation commands for rooms, items, and NPCs

use crate::ecs::components::*;
use crate::ecs::context::WorldContext;
use crate::ecs::systems::CommandResult;
use crate::ecs::EcsEntity;
use std::sync::Arc;
use uuid::Uuid;

/// Generate room description using LLM
#[tracing::instrument(skip(context), fields(entity_id = entity.id()))]
pub async fn room_generate_command(
    context: Arc<WorldContext>,
    entity: EcsEntity,
    _cmd: String,
    args: Vec<String>,
) -> CommandResult {
    tracing::debug!("Room Generate Command from {}: {}", entity.id(), args.join(" "));

    // Parse arguments
    if args.len() < 2 {
        return CommandResult::Failure(
            "Usage: room generate <uuid> <prompt>\r\n\
             Example: room generate 123e4567-e89b-12d3-a456-426614174000 a dark mysterious cave\r\n"
                .to_string(),
        );
    }

    let room_uuid = match Uuid::parse_str(&args[0]) {
        Ok(uuid) => uuid,
        Err(_) => {
            return CommandResult::Failure(format!("Invalid UUID: {}\r\n", args[0]));
        }
    };

    let prompt = args[1..].join(" ");

    // Check if LLM is available
    let llm_manager = context.llm_manager();
    let default_provider = llm_manager.get_default_provider().await;
    
    if default_provider.is_none() {
        return CommandResult::Failure(
            "LLM generation is not available. No LLM provider is configured.\r\n\
             Please configure an LLM provider (OpenAI, Ollama, or LM Studio) to use this feature.\r\n"
                .to_string(),
        );
    }

    // Get the room entity
    let room_entity = match context.get_entity_by_uuid(room_uuid).await {
        Some(e) => e,
        None => {
            return CommandResult::Failure(format!("Room {} not found.\r\n", room_uuid));
        }
    };

    // Create LLM request
    let system_prompt = "You are a creative world builder for a text-based fantasy game. \
        Generate a room name and description based on the user's prompt. \
        Respond in JSON format with fields: name (string), short_desc (string, one line), \
        long_desc (string, 2-3 sentences), keywords (array of strings).";

    let request = llm_manager
        .create_request_with_system("gpt-3.5-turbo", system_prompt, &prompt)
        .with_temperature(0.8)
        .with_max_tokens(300);

    // Send request to LLM
    match llm_manager.complete(request).await {
        Ok(response) => {
            // Parse JSON response
            match serde_json::from_str::<serde_json::Value>(&response.content) {
                Ok(json) => {
                    let name = json["name"].as_str().unwrap_or("Generated Room");
                    let short_desc = json["short_desc"].as_str().unwrap_or("A generated room.");
                    let long_desc = json["long_desc"].as_str().unwrap_or("A room generated by AI.");
                    let keywords: Vec<String> = json["keywords"]
                        .as_array()
                        .map(|arr| {
                            arr.iter()
                                .filter_map(|v| v.as_str().map(String::from))
                                .collect()
                        })
                        .unwrap_or_else(|| vec![name.to_lowercase()]);

                    // Update room components
                    let _ = context
                        .insert_one(room_entity, Name::new(name).with_keywords(keywords))
                        .await;
                    let _ = context
                        .insert_one(room_entity, Description::new(short_desc, long_desc))
                        .await;

                    // Mark as dirty for persistence
                    context.mark_entity_dirty(room_entity).await;

                    CommandResult::Success(format!(
                        "Room {} generated successfully:\r\n\
                         Name: {}\r\n\
                         Short: {}\r\n\
                         Long: {}\r\n",
                        room_uuid, name, short_desc, long_desc
                    ))
                }
                Err(_) => {
                    // Fallback: use raw response as description
                    let _ = context
                        .insert_one(room_entity, Description::new(&response.content, &response.content))
                        .await;
                    context.mark_entity_dirty(room_entity).await;

                    CommandResult::Success(format!(
                        "Room {} generated (raw response):\r\n{}\r\n",
                        room_uuid, response.content
                    ))
                }
            }
        }
        Err(e) => CommandResult::Failure(format!(
            "LLM generation failed: {:?}\r\n\
             The LLM provider may be unavailable or misconfigured.\r\n",
            e
        )),
    }
}

/// Generate item/object description using LLM
#[tracing::instrument(skip(context), fields(entity_id = entity.id()))]
pub async fn item_generate_command(
    context: Arc<WorldContext>,
    entity: EcsEntity,
    _cmd: String,
    args: Vec<String>,
) -> CommandResult {
    tracing::debug!("Item Generate Command from {}: {}", entity.id(), args.join(" "));

    // Parse arguments
    if args.len() < 2 {
        return CommandResult::Failure(
            "Usage: item generate <uuid> <prompt>\r\n\
             Example: item generate 123e4567-e89b-12d3-a456-426614174000 an ancient runed sword\r\n"
                .to_string(),
        );
    }

    let item_uuid = match Uuid::parse_str(&args[0]) {
        Ok(uuid) => uuid,
        Err(_) => {
            return CommandResult::Failure(format!("Invalid UUID: {}\r\n", args[0]));
        }
    };

    let prompt = args[1..].join(" ");

    // Check if LLM is available
    let llm_manager = context.llm_manager();
    let default_provider = llm_manager.get_default_provider().await;
    
    if default_provider.is_none() {
        return CommandResult::Failure(
            "LLM generation is not available. No LLM provider is configured.\r\n\
             Please configure an LLM provider (OpenAI, Ollama, or LM Studio) to use this feature.\r\n"
                .to_string(),
        );
    }

    // Get the item entity
    let item_entity = match context.get_entity_by_uuid(item_uuid).await {
        Some(e) => e,
        None => {
            return CommandResult::Failure(format!("Item {} not found.\r\n", item_uuid));
        }
    };

    // Create LLM request
    let system_prompt = "You are a creative item designer for a text-based fantasy game. \
        Generate an item name and description based on the user's prompt. \
        Respond in JSON format with fields: name (string), short_desc (string, one line), \
        long_desc (string, 2-3 sentences), keywords (array of strings).";

    let request = llm_manager
        .create_request_with_system("gpt-3.5-turbo", system_prompt, &prompt)
        .with_temperature(0.8)
        .with_max_tokens(300);

    // Send request to LLM
    match llm_manager.complete(request).await {
        Ok(response) => {
            // Parse JSON response
            match serde_json::from_str::<serde_json::Value>(&response.content) {
                Ok(json) => {
                    let name = json["name"].as_str().unwrap_or("Generated Item");
                    let short_desc = json["short_desc"].as_str().unwrap_or("A generated item.");
                    let long_desc = json["long_desc"].as_str().unwrap_or("An item generated by AI.");
                    let keywords: Vec<String> = json["keywords"]
                        .as_array()
                        .map(|arr| {
                            arr.iter()
                                .filter_map(|v| v.as_str().map(String::from))
                                .collect()
                        })
                        .unwrap_or_else(|| vec![name.to_lowercase()]);

                    // Update item components
                    let _ = context
                        .insert_one(item_entity, Name::new(name).with_keywords(keywords))
                        .await;
                    let _ = context
                        .insert_one(item_entity, Description::new(short_desc, long_desc))
                        .await;

                    // Mark as dirty for persistence
                    context.mark_entity_dirty(item_entity).await;

                    CommandResult::Success(format!(
                        "Item {} generated successfully:\r\n\
                         Name: {}\r\n\
                         Short: {}\r\n\
                         Long: {}\r\n",
                        item_uuid, name, short_desc, long_desc
                    ))
                }
                Err(_) => {
                    // Fallback: use raw response as description
                    let _ = context
                        .insert_one(item_entity, Description::new(&response.content, &response.content))
                        .await;
                    context.mark_entity_dirty(item_entity).await;

                    CommandResult::Success(format!(
                        "Item {} generated (raw response):\r\n{}\r\n",
                        item_uuid, response.content
                    ))
                }
            }
        }
        Err(e) => CommandResult::Failure(format!(
            "LLM generation failed: {:?}\r\n\
             The LLM provider may be unavailable or misconfigured.\r\n",
            e
        )),
    }
}

/// Generate NPC description using LLM
#[tracing::instrument(skip(context), fields(entity_id = entity.id()))]
pub async fn npc_generate_command(
    context: Arc<WorldContext>,
    entity: EcsEntity,
    _cmd: String,
    args: Vec<String>,
) -> CommandResult {
    tracing::debug!("NPC Generate Command from {}: {}", entity.id(), args.join(" "));

    // Parse arguments
    if args.len() < 2 {
        return CommandResult::Failure(
            "Usage: npc generate <uuid> <prompt>\r\n\
             Example: npc generate 123e4567-e89b-12d3-a456-426614174000 a grumpy old blacksmith\r\n"
                .to_string(),
        );
    }

    let npc_uuid = match Uuid::parse_str(&args[0]) {
        Ok(uuid) => uuid,
        Err(_) => {
            return CommandResult::Failure(format!("Invalid UUID: {}\r\n", args[0]));
        }
    };

    let prompt = args[1..].join(" ");

    // Check if LLM is available
    let llm_manager = context.llm_manager();
    let default_provider = llm_manager.get_default_provider().await;
    
    if default_provider.is_none() {
        return CommandResult::Failure(
            "LLM generation is not available. No LLM provider is configured.\r\n\
             Please configure an LLM provider (OpenAI, Ollama, or LM Studio) to use this feature.\r\n"
                .to_string(),
        );
    }

    // Get the NPC entity
    let npc_entity = match context.get_entity_by_uuid(npc_uuid).await {
        Some(e) => e,
        None => {
            return CommandResult::Failure(format!("NPC {} not found.\r\n", npc_uuid));
        }
    };

    // Create LLM request
    let system_prompt = "You are a creative character designer for a text-based fantasy game. \
        Generate an NPC name, description, personality, and dialogue prompt based on the user's input. \
        Respond in JSON format with fields: name (string), short_desc (string, one line), \
        long_desc (string, 2-3 sentences), keywords (array of strings), \
        personality (string, brief personality description), \
        dialogue_prompt (string, system prompt for NPC dialogue).";

    let request = llm_manager
        .create_request_with_system("gpt-3.5-turbo", system_prompt, &prompt)
        .with_temperature(0.8)
        .with_max_tokens(400);

    // Send request to LLM
    match llm_manager.complete(request).await {
        Ok(response) => {
            // Parse JSON response
            match serde_json::from_str::<serde_json::Value>(&response.content) {
                Ok(json) => {
                    let name = json["name"].as_str().unwrap_or("Generated NPC");
                    let short_desc = json["short_desc"].as_str().unwrap_or("A generated NPC.");
                    let long_desc = json["long_desc"].as_str().unwrap_or("An NPC generated by AI.");
                    let keywords: Vec<String> = json["keywords"]
                        .as_array()
                        .map(|arr| {
                            arr.iter()
                                .filter_map(|v| v.as_str().map(String::from))
                                .collect()
                        })
                        .unwrap_or_else(|| vec![name.to_lowercase()]);
                    let dialogue_prompt = json["dialogue_prompt"]
                        .as_str()
                        .unwrap_or("You are a helpful NPC in a fantasy world.");

                    // Update NPC components
                    let _ = context
                        .insert_one(npc_entity, Name::new(name).with_keywords(keywords))
                        .await;
                    let _ = context
                        .insert_one(npc_entity, Description::new(short_desc, long_desc))
                        .await;

                    // Add or update NPC dialogue configuration
                    let dialogue = NpcDialogue::new("gpt-3.5-turbo")
                        .with_system_prompt(dialogue_prompt)
                        .with_llm_enabled(false); // Disabled by default, can be enabled later

                    let _ = context.insert_one(npc_entity, dialogue).await;

                    // Mark as dirty for persistence
                    context.mark_entity_dirty(npc_entity).await;

                    CommandResult::Success(format!(
                        "NPC {} generated successfully:\r\n\
                         Name: {}\r\n\
                         Short: {}\r\n\
                         Long: {}\r\n\
                         Dialogue prompt configured (LLM disabled by default)\r\n",
                        npc_uuid, name, short_desc, long_desc
                    ))
                }
                Err(_) => {
                    // Fallback: use raw response as description
                    let _ = context
                        .insert_one(npc_entity, Description::new(&response.content, &response.content))
                        .await;
                    context.mark_entity_dirty(npc_entity).await;

                    CommandResult::Success(format!(
                        "NPC {} generated (raw response):\r\n{}\r\n",
                        npc_uuid, response.content
                    ))
                }
            }
        }
        Err(e) => CommandResult::Failure(format!(
            "LLM generation failed: {:?}\r\n\
             The LLM provider may be unavailable or misconfigured.\r\n",
            e
        )),
    }
}


